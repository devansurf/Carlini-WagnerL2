{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/devansurf/Carlini-WagnerL2/blob/main/Carlini%26Wagner_L2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCW3TkPz1mrJ"
      },
      "source": [
        "# Carlini & Wagner (L2) Adversarial Attack\n",
        "Load pretrained models from HuggingFace, run attacks and plot images and any relevant metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IurJFq3UMRpQ",
        "outputId": "eb2a4d2e-58de-4f28-9e6e-a2dd59ccbc92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchattacks\n",
            "  Downloading torchattacks-3.5.1-py3-none-any.whl (142 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/142.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m133.1/142.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.0/142.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from torchattacks) (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.10/dist-packages (from torchattacks) (0.17.1+cu121)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from torchattacks) (1.11.4)\n",
            "Requirement already satisfied: tqdm>=4.56.1 in /usr/local/lib/python3.10/dist-packages (from torchattacks) (4.66.2)\n",
            "Collecting requests~=2.25.1 (from torchattacks)\n",
            "  Downloading requests-2.25.1-py2.py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.19.4 in /usr/local/lib/python3.10/dist-packages (from torchattacks) (1.25.2)\n",
            "Collecting chardet<5,>=3.0.2 (from requests~=2.25.1->torchattacks)\n",
            "  Downloading chardet-4.0.0-py2.py3-none-any.whl (178 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.7/178.7 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting idna<3,>=2.5 (from requests~=2.25.1->torchattacks)\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting urllib3<1.27,>=1.21.1 (from requests~=2.25.1->torchattacks)\n",
            "  Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests~=2.25.1->torchattacks) (2024.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.7.1->torchattacks)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.7.1->torchattacks)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.7.1->torchattacks)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.7.1->torchattacks)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.7.1->torchattacks)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.7.1->torchattacks)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.7.1->torchattacks)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.7.1->torchattacks)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.7.1->torchattacks)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.7.1->torchattacks)\n"
          ]
        }
      ],
      "source": [
        "%pip install torchattacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dlaAwF54yZCT"
      },
      "outputs": [],
      "source": [
        "%pip install accelerate -U"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SjzgEOC_XnW"
      },
      "source": [
        "IMPORTS!!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmjZnyxl_UXt"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.models as models\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torchattacks\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import xml.etree.ElementTree as ET\n",
        "import os\n",
        "\n",
        "from transformers import TrainingArguments, Trainer, default_data_collator\n",
        "from transformers import ViTImageProcessor, BeitImageProcessor, ConvNextImageProcessor\n",
        "from transformers import ViTForImageClassification, AutoModelForImageClassification\n",
        "from transformers import ViTFeatureExtractor, BeitFeatureExtractor, ConvNextFeatureExtractor\n",
        "\n",
        "from PIL import Image\n",
        "from torch.utils.data import DataLoader, Dataset, Subset\n",
        "from sklearn.metrics import classification_report, f1_score, confusion_matrix\n",
        "\n",
        "from torch import nn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vAt8YLpo9bR"
      },
      "outputs": [],
      "source": [
        "# CPU or GPU\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)\n",
        "\n",
        "using_kaggle = False\n",
        "using_torch = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbbzFVL-V4X5"
      },
      "outputs": [],
      "source": [
        "#Confusion matrix\n",
        "def generate_confusion_matrix(actual_labels, predicted_labels, classes):\n",
        "    cm = confusion_matrix(actual_labels, predicted_labels)\n",
        "\n",
        "\n",
        "    fig = plt.figure(figsize=(16, 14))\n",
        "    ax = plt.subplot()\n",
        "    sns.heatmap(cm, annot=True, ax=ax, fmt=\"g\")\n",
        "    ax.set_xlabel(\"Predicted\", fontsize=20)\n",
        "    ax.xaxis.set_label_position(\"bottom\")\n",
        "    plt.xticks(rotation=90)\n",
        "    ax.xaxis.tick_bottom()\n",
        "\n",
        "    ax.set_ylabel(\"True\", fontsize=20)\n",
        "    plt.yticks(rotation=0)\n",
        "\n",
        "    plt.title(\"Confusion Matrix\", fontsize=20)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    F1_score = f1_score(actual_labels, predicted_labels, average='macro')\n",
        "\n",
        "    print(f'f1_score: {F1_score}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9VYJ78ABkt1l"
      },
      "outputs": [],
      "source": [
        "def plot_images(x, y, class_names, x_adv=None, y_adv=None):\n",
        "    if x_adv is not None:\n",
        "        fig, ax = plt.subplots(3, 10, figsize=(20, 5))\n",
        "    else:\n",
        "        fig, ax = plt.subplots(1, 10, figsize=(20, 5))\n",
        "\n",
        "    for i in range(10):\n",
        "        if x_adv is not None:\n",
        "            ax[0, i].imshow(x[i].permute(1, 2, 0))\n",
        "            ax[0, i].set_title(class_names[y[i]])\n",
        "            ax[0, i].axis(\"off\")\n",
        "            ax[1, i].imshow(x_adv[i].detach().cpu().permute(1, 2, 0))\n",
        "            if y_adv[i].item() != y[i].item():\n",
        "                ax[1, i].set_title(class_names[y_adv[i]], color=\"r\")\n",
        "            else:\n",
        "                ax[1, i].set_title(class_names[y_adv[i]], color=\"k\")\n",
        "            ax[1, i].axis(\"off\")\n",
        "            ax[2, i].imshow(\n",
        "                (torch.abs(x[i] - x_adv[i].detach().cpu()) * 25)\n",
        "                .clamp_(0, 1)\n",
        "                .permute(1, 2, 0)\n",
        "            )\n",
        "            ax[2, i].axis(\"off\")\n",
        "        else:\n",
        "            ax[i].imshow(x[i].permute(1, 2, 0))\n",
        "            ax[i].set_title(class_names[y[i]])\n",
        "            ax[i].axis(\"off\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rK6bjdfRgVOQ"
      },
      "outputs": [],
      "source": [
        "class HuggingFaceModelWrapper(torch.nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super(HuggingFaceModelWrapper, self).__init__()\n",
        "        # Load the pretrained model\n",
        "\n",
        "        self.model = model\n",
        "        #Hardcoded output layer (That of CIFAR10 num_classes)\n",
        "        #self.model.classifier = nn.Linear(self.model.classifier.in_features, 10)\n",
        "\n",
        "    def forward(self, pixel_values):\n",
        "        if not isinstance(pixel_values, torch.Tensor):\n",
        "            raise ValueError(\"pixel_values must be a torch.Tensor\")\n",
        "\n",
        "        # Hugging Face models expect a dictionary with 'pixel_values' as the key\n",
        "        outputs = self.model(pixel_values=pixel_values)\n",
        "\n",
        "        # Extract logits\n",
        "        logits = outputs.logits\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4QfztdaJ5z7"
      },
      "source": [
        "HuggingFace pretrained models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zmbjaiy0J740"
      },
      "outputs": [],
      "source": [
        "# ViT model\n",
        "model_name_vit = \"google/vit-base-patch16-224-in21k\"\n",
        "feature_extractor_vit = ViTFeatureExtractor.from_pretrained(model_name_vit)\n",
        "model_vit = ViTForImageClassification.from_pretrained(model_name_vit, num_labels=10)\n",
        "\n",
        "# BEiT model\n",
        "# model_name_beit = \"microsoft/beit-base-patch16-224-pt22k-ft22k\"\n",
        "# feature_extractor_beit = BeitFeatureExtractor.from_pretrained(model_name_beit)\n",
        "# model_beit = AutoModelForImageClassification.from_pretrained(model_name_beit)\n",
        "\n",
        "# ConvNeXt model\n",
        "# model_name_convnext = \"facebook/convnext-base-224\"\n",
        "# feature_extractor_convnext = ConvNextFeatureExtractor.from_pretrained(model_name_convnext)\n",
        "# model_convnext = AutoModelForImageClassification.from_pretrained(model_name_convnext, ignore_mismatched_sizes = True, num_labels=10)\n",
        "\n",
        "# model_vgg = models.vgg16(pretrained=True)\n",
        "model_vgg.classifier[6] = torch.nn.Linear(4096, 10)  # Adjusting the last layer for 10 classes\n",
        "\n",
        "# if using_kaggle:\n",
        "#     model_convnext = AutoModelForImageClassification.from_pretrained(model_name_convnext, ignore_mismatched_sizes = True, num_labels=4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvQPekNDLrSi"
      },
      "source": [
        "HuggingFace Wrapper function to handle outputs for torchattacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CkjIoZS0YBhG"
      },
      "outputs": [],
      "source": [
        "if using_kaggle:\n",
        "    !pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcJ4nO_GotGb"
      },
      "outputs": [],
      "source": [
        "# We create a hidden directory to store our kaggle API key that would let us run\n",
        "# everything else\n",
        "from google.colab import files\n",
        "\n",
        "if using_kaggle:\n",
        "    files.upload()\n",
        "\n",
        "    !mkdir -p ~/.kaggle\n",
        "\n",
        "    !cp kaggle.json ~/.kaggle/\n",
        "\n",
        "    !chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fuqCiAh0YIdz"
      },
      "outputs": [],
      "source": [
        "if using_kaggle:\n",
        "    !kaggle datasets download -d andrewmvd/road-sign-detection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gp7-kUGIYJgx"
      },
      "outputs": [],
      "source": [
        "if using_kaggle:\n",
        "    !unzip road-sign-detection.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3PIkhkntuTJ"
      },
      "source": [
        "#Gather images and annotations from Kaggle dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSEA6G5FWjDs"
      },
      "outputs": [],
      "source": [
        "#Dataset focused on roadsigns\n",
        "class KaggleHuggingFace(Dataset):\n",
        "    def __init__(self, img_dir, annotations_dir, split='train'):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img_dir (string): Path to the images directory.\n",
        "            annotations_dir (string): Path to the annotations directory.\n",
        "            split (string, optional): Indicates if the dataset is for training or testing. Defaults to 'train'.\n",
        "            transform (callable, optional): Optional transform to be applied on a sample.\n",
        "        \"\"\"\n",
        "        self.img_dir = img_dir\n",
        "        self.annotations_dir = annotations_dir\n",
        "\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)), #model input size\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "\n",
        "        self.split = split\n",
        "        self.images = os.listdir(img_dir)\n",
        "        self.labels = self._extract_labels()\n",
        "\n",
        "        # Split dataset into train/test if necessary\n",
        "        if self.split == 'train':\n",
        "            self.images, self.labels = self._split_dataset(self.images, self.labels, train=True)\n",
        "        else:\n",
        "            self.images, self.labels = self._split_dataset(self.images, self.labels, train=False)\n",
        "\n",
        "        self.classes = sorted(list(set(self.labels.values())))\n",
        "\n",
        "    def _extract_labels(self):\n",
        "        labels = {}\n",
        "        for annotation in os.listdir(self.annotations_dir):\n",
        "            xml_path = os.path.join(self.annotations_dir, annotation)\n",
        "            tree = ET.parse(xml_path)\n",
        "            root = tree.getroot()\n",
        "            filename = root.find('filename').text\n",
        "            object_name = root.find('.//object/name').text  # Assumes one main object per image\n",
        "            labels[filename] = object_name\n",
        "        return labels\n",
        "\n",
        "    def _split_dataset(self, images, labels, train=True, train_size=0.8):\n",
        "        # Split images based on split flag\n",
        "        split_idx = int(len(images) * train_size)\n",
        "        if train:\n",
        "            selected_images = images[:split_idx]\n",
        "        else:\n",
        "            selected_images = images[split_idx:]\n",
        "\n",
        "        # Filter labels to match selected images\n",
        "        selected_labels = {k: v for k, v in labels.items() if k in selected_images}\n",
        "        return selected_images, selected_labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.images[idx]\n",
        "        img_path = os.path.join(self.img_dir, img_name)\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        label = self.labels[img_name]\n",
        "        label_idx = self.classes.index(label)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        if self.split == 'test':\n",
        "            return image, label_idx\n",
        "        return {'pixel_values': image, 'labels': label_idx}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FrF2OJteLpMu"
      },
      "outputs": [],
      "source": [
        "class CIFAR10HuggingFace(Dataset):\n",
        "    def __init__(self, split='train', subset_size=1000):\n",
        "        self.split = split\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)), #model input size\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "\n",
        "         # Load the full dataset\n",
        "        self.dataset = datasets.CIFAR10(root='./data', train=(split == 'train'),\n",
        "                                        download=True, transform=self.transform)\n",
        "        self.classes = self.dataset.classes\n",
        "\n",
        "        if (split == 'train'):\n",
        "            # Generate a random subset of indices\n",
        "            if subset_size > len(self.dataset):\n",
        "                raise ValueError(\"Subset size is larger than the dataset size.\")\n",
        "            subset_indices = np.random.choice(len(self.dataset), subset_size, replace=False)\n",
        "\n",
        "            # Create a dataset subset\n",
        "            self.dataset = Subset(self.dataset, subset_indices)\n",
        "\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, label = self.dataset[idx]\n",
        "        return {'pixel_values': image, 'labels': label}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dksJjpRY3nXu"
      },
      "outputs": [],
      "source": [
        "class TorchModelWrapper(nn.Module):\n",
        "    def __init__(self, model, num_classes=10):\n",
        "        super(TorchModelWrapper, self).__init__()\n",
        "        self.model = model\n",
        "        self.model.classifier[6] = nn.Linear(self.model.classifier[6].in_features, num_classes)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        pixel_values = inputs['pixel_values']\n",
        "        outputs = self.model(pixel_values)\n",
        "        return {'logits': outputs}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3zcJA6PxS35"
      },
      "outputs": [],
      "source": [
        "#Fine-Tune model\n",
        "train_dataset = CIFAR10HuggingFace(split='train', subset_size=10000)\n",
        "eval_dataset = CIFAR10HuggingFace(split='test')\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False,\n",
        "            download=True,\n",
        "            transform=transforms.Compose([\n",
        "            transforms.Resize((224, 224)), #model input size\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        ")\n",
        "\n",
        "if using_kaggle:\n",
        "    train_dataset = KaggleHuggingFace('/content/images', '/content/annotations', split='train')\n",
        "    eval_dataset = KaggleHuggingFace('/content/images', '/content/annotations', split='validation')\n",
        "    test_dataset = KaggleHuggingFace('/content/images', '/content/annotations', split='test')\n",
        "\n",
        "def train_model(model, feature_extractor = None):\n",
        "    model = model.to(device)\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir='./results',          # Where to store the model checkpoints\n",
        "        num_train_epochs=9,              # Number of training epochs\n",
        "        per_device_train_batch_size=16,   # Training batch size\n",
        "        per_device_eval_batch_size=16,    # Evaluation batch size\n",
        "        warmup_steps=500,                # Number of warmup steps\n",
        "        weight_decay=0.01,               # Weight decay\n",
        "        logging_dir='./logs',            # Where to store logs\n",
        "        logging_steps=10,\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        data_collator=default_data_collator, #not usually used in Image Processing, but used mostly in NLP's\n",
        "        tokenizer=feature_extractor,\n",
        "    )\n",
        "    trainer.train()\n",
        "    trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VFGPLYa4MqY"
      },
      "outputs": [],
      "source": [
        "def torch_train(model, learning_rate = 0.001, epochs = 10, batch_size = 32):\n",
        "\n",
        "    train_dataset = datasets.CIFAR10(root='./data', train=True,\n",
        "            download=True,\n",
        "            transform=transforms.Compose([\n",
        "            transforms.Resize((224, 224)), #model input size\n",
        "            transforms.ToTensor(),\n",
        "        ]))\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Optimizer and loss function\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader):.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "l90RtsQ8dyN3"
      },
      "outputs": [],
      "source": [
        "#Train all models\n",
        "\n",
        "train_model(model_vit, feature_extractor_vit)\n",
        "model_vit = HuggingFaceModelWrapper(model_vit)\n",
        "\n",
        "# train_model(model_beit, feature_extractor_beit)\n",
        "# model_beit = HuggingFaceModelWrapper(model_beit)\n",
        "\n",
        "# train_model(model_convnext, feature_extractor_convnext)\n",
        "# model_convnext = HuggingFaceModelWrapper(model_convnext)\n",
        "# torch_train(model_vgg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSz3KSw1gV_U"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yKzSkRWmn2J"
      },
      "source": [
        "Lets start making some predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1roVrBOae3MO"
      },
      "outputs": [],
      "source": [
        "def run_attack(model):\n",
        "    #atk = torchattacks.CW(model.to(device), c=1, steps=1000, lr=0.01)\n",
        "    atk = torchattacks.OnePixel(model, pixels=10, steps=10, popsize=10, inf_batch=128)\n",
        "    dataloader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
        "\n",
        "    for image, label in dataloader:\n",
        "        image, label = image.to(device), label.to(device)\n",
        "\n",
        "        #make prediction without using adversarial images\n",
        "        with torch.no_grad():\n",
        "            predictions = model(image)\n",
        "        predicted_label = torch.argmax(predictions, dim=1)\n",
        "\n",
        "        plot_images(image.cpu(), predicted_label.cpu(), test_dataset.classes)\n",
        "        generate_confusion_matrix(predicted_label.cpu(), label.cpu(), test_dataset.classes)\n",
        "\n",
        "        # Create adversarial images\n",
        "        adv_image = atk(image,label)\n",
        "\n",
        "        # Make predictions using adversarial images\n",
        "        with torch.no_grad():\n",
        "            predictions = model(adv_image)\n",
        "        adv_label = torch.argmax(predictions, dim=1)\n",
        "\n",
        "\n",
        "        #plot the results\n",
        "        plot_images(image.cpu(), label.cpu(), test_dataset.classes, x_adv=adv_image.cpu(), y_adv=adv_label.cpu())\n",
        "        generate_confusion_matrix(adv_label.cpu(), label.cpu(), test_dataset.classes)\n",
        "\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9IijuhUiugu"
      },
      "outputs": [],
      "source": [
        "run_attack(model_vit)\n",
        "#run_attack(model_beit)\n",
        "#run_attack(model_convnext)\n",
        "#run_attack(model_vgg, learning_rate = 0.001, epochs = 10, batch_size = 32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJcUn4GYl1Vi"
      },
      "source": [
        "Function that helps plot images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvqLyF90mIBi"
      },
      "source": [
        "Lets use our previously defined function (taken from MITLL) to plot our adversarial images"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "mount_file_id": "11g-icpZBQYkVP4xzZ0hPgsJe5ALN9g4D",
      "authorship_tag": "ABX9TyOVkU7wbaZ3YKAkQ5GR0r8x",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}